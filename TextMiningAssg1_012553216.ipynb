{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3ABC278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "input_str = 'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3ABC278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.com'\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting snetences from the text\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokens_sent = tokenizer.tokenize(input_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use regular expressions to extract all license plate numbers, IDs, email and mailing addresses from the following text: 'I am 20 years old. My previous license plate number was 4XUI302 and my new one is 3ABC278. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "License Plates: ['4XUI302', '3ABC278']\n",
      "ID: J987492\n",
      "Mailing Address: 123 Main street, San Jose, CA.\n",
      "Email: myemail123+spam@google.com\n"
     ]
    }
   ],
   "source": [
    "rgx = r'[\\w\\-_+#~!$&\\'\\.]+@[\\w\\.-]+'\n",
    "rgx2 = r'(\\d+\\w+\\d+)'\n",
    "rgx3 = r'(\\w\\d+)'\n",
    "rgx4 = r'\\d{1,4} +\\w+ \\w+\\, \\w+ \\w+, \\w+.'\n",
    "for index,val in enumerate(tokens_sent):\n",
    "    match = re.findall(rgx, val)\n",
    "    if match:\n",
    "        print(\"Email:\",match[0])\n",
    "    if val.find('license') != -1:\n",
    "        match_license = re.findall(rgx2,val)\n",
    "        print('License Plates:',match_license)\n",
    "    if val.find('ID') != -1:\n",
    "        match_id = re.findall(rgx3,val)\n",
    "        print('ID:',match_id[0])\n",
    "        add = val.split('address')[1]\n",
    "        print('Mailing Address:',re.findall(rgx4,add)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace all license plate numbers with the abbreviation \"LP_NUM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am 20 years old. My previous license plate number was LP_NUM and my new one is LP_NUM. My ID is J987492 and my address is 123 Main street, San Jose, CA. Please email me at myemail123+spam@google.com\n"
     ]
    }
   ],
   "source": [
    "# replace license plate number with abbreviation\n",
    "for i, license in enumerate(match_license):\n",
    "    if input_str.find(license) != -1:\n",
    "        input_str = input_str.replace(license,\"LP_NUM\",1)\n",
    "print( input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic text operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]\n",
      "1583820\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "print(movie_reviews.words())\n",
    "print(len(movie_reviews.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = movie_reviews.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation from movie_reviews.words() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', 'two', 'teen', 'couples', 'go']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "words_wo_punc = [word for word in words if word not in string.punctuation]\n",
    "print(words_wo_punc[:5]) # printing first five words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many unique words are in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39737\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(len(Counter(words_wo_punc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the top 20 most frequently encountered words in the corpus? (e.g., use FreqDist from nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 76529), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), ('is', 25195), ('in', 21822), ('s', 18513), ('it', 16107), ('that', 15924), ('as', 11378), ('with', 10792), ('for', 9961), ('his', 9587), ('this', 9578), ('film', 9517), ('i', 8889), ('he', 8864), ('but', 8634), ('on', 7385)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "freq = FreqDist(words_wo_punc)\n",
    "print(freq.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_english = stopwords.words('english')\n",
    "words_without_stopwords = [word for word in words_wo_punc if word not in stopwords_english]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many unique words are in the corpus after you've removed the stop words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39586\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(len(set(words_without_stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the top 20 most frequently encountered words in the corpus? (e.g., use FreqDist from nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 9517), ('one', 5852), ('movie', 5771), ('like', 3690), ('even', 2565), ('good', 2411), ('time', 2411), ('story', 2169), ('would', 2109), ('much', 2049), ('character', 2020), ('also', 1967), ('get', 1949), ('two', 1911), ('well', 1906), ('characters', 1859), ('first', 1836), ('--', 1815), ('see', 1749), ('way', 1693)]\n"
     ]
    }
   ],
   "source": [
    "freq_stop_words = FreqDist(words_without_stopwords)\n",
    "print(freq_stop_words.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the words that are used only once in the corpus (show the first 20 words returned by the hapaxes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['looooot',\n",
       " 'schnazzy',\n",
       " 'timex',\n",
       " 'indiglo',\n",
       " 'jessalyn',\n",
       " 'gilsig',\n",
       " 'ruber',\n",
       " 'jaleel',\n",
       " 'balki',\n",
       " 'wavers',\n",
       " 'statistics',\n",
       " 'snapshot',\n",
       " 'guesswork',\n",
       " 'maryam',\n",
       " 'daylights',\n",
       " 'terraformed',\n",
       " 'stagnated',\n",
       " 'napolean',\n",
       " 'millimeter',\n",
       " 'enmeshed']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_stop_words.hapaxes()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the following operations on the corpus that resulted after undergoing the processing in Step 2 (i.e., punctuation has been removed and stop words have been removed):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_no_stop_punc = [word.lower() for word in words_without_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemm_words = [stemmer.stem(word) for word in words_without_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after stemming: 26101\n"
     ]
    }
   ],
   "source": [
    "print('Unique words after stemming:',len(Counter(stemm_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words after lemmatization: 35172\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemm_words = [lemmatizer.lemmatize(word) for word in words_no_stop_punc]\n",
    "print('Unique words after lemmatization:',len(set(lemm_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
